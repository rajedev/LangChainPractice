{
 "cells": [
  {
   "cell_type": "code",
   "id": "d812c2e81f813a85",
   "metadata": {},
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39d17bc65c1c83cf",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34f4a5dec6f6e8da",
   "metadata": {},
   "source": [
    "@tool(\"admission_seat_count\", description=\"This tells the student count of department wise\")\n",
    "def admission_seat_count():\n",
    "    return {\n",
    "        \"electronics\":23,\n",
    "        \"computer_science\":45,\n",
    "        \"mechanical\":60,\n",
    "        \"english\":2\n",
    "    }\n",
    "\n",
    "@tool(\"faculty_openings\", description=\"this tells the no. of position opening for faculty based on the major degree\")\n",
    "def faculty_openings():\n",
    "    return {\n",
    "        \"technology\":3,\n",
    "        \"engineering\":5,\n",
    "        \"master\":4\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "103c2f2ee7eb045d",
   "metadata": {},
   "source": [
    "agent = create_agent(model=llm, system_prompt=\"You are an intelligent assistant help us on the questions related to the college admissions and faculty openings, please precise with your answer no need to share other information.\", tools=[admission_seat_count, faculty_openings])\n",
    "\n",
    "response = agent.invoke({\n",
    "    \"messages\":[\n",
    "        HumanMessage(content=\"Tell me no of student seats available for english? and no of professors positions open for engineering\")\n",
    "    ]\n",
    "}\n",
    ")\n",
    "print(response[\"messages\"][-1].content)\n",
    "agent\n",
    "#result = llm.invoke(\"Tell me details on python jupiter notebook in 2 lines?\")\n",
    "#result.content_blocks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5dc8a0c0900326a",
   "metadata": {},
   "source": [
    "agent1 = create_agent(model=llm, system_prompt=\"You are an intelligent ai assistant to answer user queries\")\n",
    "\n",
    "# Correct way to stream with agents\n",
    "response = agent1.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"tell me on langchain middleware in 200 words?\")]},\n",
    "    stream_mode=\"messages\"\n",
    ")\n",
    "print(response)\n",
    "# print(type(response))\n",
    "# for chunk, metadata in response:\n",
    "#     # chunk is the message, metadata contains info about which node produced it\n",
    "#     if hasattr(chunk, 'content') and chunk.content:\n",
    "#         print(chunk.content, end=\"\", flush=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ace11246",
   "metadata": {},
   "source": [
    "# Alternative streaming approaches\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Method 1: Stream mode 'values' (recommended for agents)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "response = agent1.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"What is LangChain in 50 words?\")]},\n",
    "    stream_mode=\"values\"\n",
    ")\n",
    "\n",
    "print(response)\n",
    "# for state in response:\n",
    "#     # Get the last message from the state\n",
    "#     if \"messages\" in state and state[\"messages\"]:\n",
    "#         last_msg = state[\"messages\"][-1]\n",
    "#         if hasattr(last_msg, 'content'):\n",
    "#             print(last_msg.content)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"Method 2: Stream mode 'updates' (shows incremental updates)\")\n",
    "# print(\"=\"*60)\n",
    "#\n",
    "# response = agent1.stream(\n",
    "#     {\"messages\": [HumanMessage(content=\"Explain Python in 30 words?\")]},\n",
    "#     stream_mode=\"updates\"\n",
    "# )\n",
    "#\n",
    "# for update in response:\n",
    "#     print(update)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"Method 3: Direct LLM streaming (no agent wrapper)\")\n",
    "# print(\"=\"*60)\n",
    "#\n",
    "# # If you just want to stream LLM responses without agent logic\n",
    "# for chunk in llm.stream(\"What is machine learning in 40 words?\"):\n",
    "#     print(chunk.content, end=\"\", flush=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ec1c9e7",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "STREAMING WITH LANGCHAIN AGENTS - EXPLANATION\n",
    "\n",
    "The issue: agent.stream() with stream_mode=\"messages\" returns TUPLES, not message objects!\n",
    "\n",
    "Format: (message_chunk, metadata_dict)\n",
    "\n",
    "Common stream_mode options:\n",
    "1. \"messages\" - Returns (chunk, metadata) tuples for each message chunk\n",
    "2. \"values\" - Returns complete state after each step (easier to use)\n",
    "3. \"updates\" - Returns only the updates/changes to state\n",
    "\n",
    "Best practices:\n",
    "- For token-by-token streaming: Use stream_mode=\"messages\" and unpack tuples\n",
    "- For step-by-step results: Use stream_mode=\"values\"\n",
    "- For direct LLM streaming (no agent): Use llm.stream() directly\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
